{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-28T09:55:17.597731Z",
     "start_time": "2025-08-28T09:55:17.559953Z"
    }
   },
   "source": "print(\"test\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T07:22:04.555004Z",
     "start_time": "2025-08-29T07:17:13.298597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# 1. Stała wartość seed\n",
    "seed = 42\n",
    "\n",
    "# 2. Ustawienie seedów dla reprodukowalności\n",
    "os.environ[\"PYTHONHASHseed\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# 3. Wymuszenie deterministycznych operacji w TensorFlow\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# 4. Parametryzacja\n",
    "train_dir = \"plants_train\"\n",
    "val_dir = \"plants_test\"\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255,        # Skaluje wartości pikseli z zakresu 0-255 na zakres 0-1 (taki zakres jest łatwiejszy do obróbki dla sieci neuronowych)\n",
    "    rotation_range = 20,        # Losowo obraca obrazy o kąt od -20 do 20 stopni (pomaga modelowi nauczyć się rozpoznawać obiekty pod różnymi kątami)\n",
    "    zoom_range = 0.2,           # Losowo przybliża obrazy o maksymalnie 20% (symulacja różnych odległości od obiektu)\n",
    "    horizontal_flip = True,     # Losowo odbija obrazy poziomo (pomaga modelowi uczyć się, że kierunek nie ma znaczenia)\n",
    "    validation_split = 0.2      # Dzieli dane na dane treningowe i walidacyjne (20% danych na walidację, 80% na trening)\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# 1. base_model = MobileNetV2(...):\n",
    "# MobileNetV2 to funkcja, która tworzy model MobileNetV2 w Keras. W tym przypadku tworzysz model, który będzie pełnił rolę \"bazy\", na której będziesz budował resztę swojego modelu.\n",
    "base_model = MobileNetV2(\n",
    "\n",
    "    # 2. weights = \"imagenet\":\n",
    "    # Używasz pretrenowanych wag z ImageNet. ImageNet to ogromny zbiór danych zawierający miliony obrazów z różnymi klasami obiektów (np. psy, koty, samochody itd.). Model MobileNetV2 jest już wytrenowany na tym zbiorze, co oznacza, że zna już podstawowe cechy wizualne, takie jak kształty, kolory, tekstury, itp.\n",
    "    # Dzięki temu, zamiast trenować model od podstaw (co jest bardzo czasochłonne), możesz wykorzystać tę wcześniej zdobytą wiedzę i dostosować model do swojego zadania.\n",
    "    weights = \"imagenet\",\n",
    "\n",
    "# -------\n",
    "    # 3. include_top = False:\n",
    "    # include_top mówi, czy chcesz, aby na końcu modelu znajdowały się warstwy odpowiedzialne za klasyfikację (tzw. \"top layers\").\n",
    "    # False oznacza, że usuwasz te warstwy. W rezultacie model MobileNetV2 będzie miał tylko część \"feature extraction\" (czyli warstwy odpowiedzialne za wydobywanie cech z obrazów), ale nie będzie końcowej warstwy, która wykonuje klasyfikację.\n",
    "    # Robisz to, ponieważ chcesz dostosować model do własnych danych. Zamiast korzystać z domyślnej klasyfikacji dla ImageNet, chcesz dodać własne warstwy klasyfikacyjne (np. z inną liczbą klas, odpowiadającą Twojemu zbiorowi danych).\n",
    "    include_top = False,\n",
    "\n",
    "# -------\n",
    "    # 4. input_shape = (*IMG_SIZE, 3):\n",
    "    # input_shape ustawia kształt danych wejściowych, które model będzie przyjmować.\n",
    "    # *IMG_SIZE to rozmiar obrazów, który określiłeś wcześniej (224x224).\n",
    "    # Oznacza to, że obrazy wejściowe będą miały wymiary 224x224.\n",
    "    # 3 na końcu oznacza liczbę kanałów kolorów w obrazie (3 = RGB) - oznacza to że są to obrazki kolorowe. Więc obraz wejściowy będzie miał wymiary 224x224x3 (wysokość, szerokość, liczba kanałów kolorów).\n",
    "    input_shape = (*IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# -------\n",
    "# 5. base_model.trainable = False:\n",
    "# Po załadowaniu modelu, ustawiasz trainable = False dla warstw bazy (MobileNetV2).\n",
    "# Oznacza to, że nie będziesz trenować wag wstępnie wytrenowanego modelu.\n",
    "# Jest to częsta praktyka w transfer learning. Kiedy korzystasz z pretrenowanego modelu, na ogół nie trenujesz go od nowa, ponieważ ma on już wyuczoną wiedzę o podstawowych cechach wizualnych.\n",
    "# Dzięki temu model będzie działał szybciej, ponieważ tylko Twoje dodatkowe warstwy (które dodasz później) będą trenowane, a nie cały model.\n",
    "# Umożliwia to również oszczędność pamięci i zmniejsza ryzyko przeuczenia.\n",
    "base_model.trainable = False\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bierzemy wyjście z warstwy bazowego modelu (MobileNetV2), które zawiera cechy obrazu\n",
    "x = base_model.output\n",
    "\n",
    "# Warstwa GlobalAveragePooling2D wykonuje globalne średnie pooling dla każdego kanału (filtra) obrazu\n",
    "# Zmienia wymiary z (wysokość, szerokość, liczba filtrów) na (liczba filtrów)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Warstwa Dense wprowadza dodatkową abstrakcję, gdzie mamy 128 neuronów.\n",
    "# Funkcja aktywacji Leaky ReLU pozwala na mały przepływ wartości ujemnych, unikając martwych neuronów.\n",
    "x = Dense(128, activation = \"leaky_relu\")(x)\n",
    "\n",
    "# Warstwa Dense, która odpowiada za klasyfikację.\n",
    "# Liczba neuronów to liczba klas w danych (np. 12 klas roślin).\n",
    "# Funkcja aktywacji Softmax daje prawdopodobieństwo dla każdej klasy.\n",
    "outputs = Dense(\n",
    "    train_generator.num_classes,  # Liczba klas\n",
    "    activation = \"softmax\"        # Daje prawdopodobienstwa\n",
    ")(x)\n",
    "\"\"\"\n",
    "test\n",
    "\"\"\"\n",
    "# Tworzymy pełny model, który łączy wejście z bazowego modelu (MobileNetV2) oraz wyjście z warstwy klasyfikacyjnej\n",
    "model = Model(\n",
    "    inputs = base_model.input,  # Wejście modelu to wejście z bazowego modelu (MobileNetV2)\n",
    "    outputs = outputs           # Wyjście modelu to wynik z ostatniej warstwy klasyfikacji\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data = val_generator,\n",
    "    epochs = 5\n",
    ")"
   ],
   "id": "4f3e19773805f2d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 images belonging to 12 classes.\n",
      "Found 216 images belonging to 12 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1s/step - accuracy: 0.2539 - loss: 2.3549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grzegorz\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 2s/step - accuracy: 0.3449 - loss: 2.0077 - val_accuracy: 0.5185 - val_loss: 1.3493\n",
      "Epoch 2/5\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m54s\u001B[0m 2s/step - accuracy: 0.5810 - loss: 1.2212 - val_accuracy: 0.6389 - val_loss: 1.1188\n",
      "Epoch 3/5\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m58s\u001B[0m 2s/step - accuracy: 0.6597 - loss: 1.0001 - val_accuracy: 0.6759 - val_loss: 1.0105\n",
      "Epoch 4/5\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 2s/step - accuracy: 0.6991 - loss: 0.8402 - val_accuracy: 0.7037 - val_loss: 0.9111\n",
      "Epoch 5/5\n",
      "\u001B[1m27/27\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m58s\u001B[0m 2s/step - accuracy: 0.7488 - loss: 0.7120 - val_accuracy: 0.6759 - val_loss: 0.9369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23936bc1fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
